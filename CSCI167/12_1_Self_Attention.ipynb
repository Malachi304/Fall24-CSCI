{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9vk9Elugvmi"
   },
   "source": [
    "# **Notebook 12.1: Self Attention**\n",
    "\n",
    "This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n",
    "\n",
    "Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OLComQyvCIJ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OJkkoNqCVK2"
   },
   "source": [
    "The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oAygJwLiCSri"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.78862847],\n",
      "       [ 0.43650985],\n",
      "       [ 0.09649747],\n",
      "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
      "       [-0.35475898],\n",
      "       [-0.08274148],\n",
      "       [-0.62700068]]), array([[-0.04381817],\n",
      "       [-0.47721803],\n",
      "       [-1.31386475],\n",
      "       [ 0.88462238]])]\n"
     ]
    }
   ],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(3)\n",
    "# Number of inputs\n",
    "N = 3\n",
    "# Number of dimensions of each input\n",
    "D = 4\n",
    "# Create an empty list\n",
    "all_x = []\n",
    "# Create elements x_n and append to list\n",
    "for n in range(N):\n",
    "  all_x.append(np.random.normal(size=(D,1)))\n",
    "# Print out the list\n",
    "print(all_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2iHFbtKMaDp"
   },
   "source": [
    "We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "79TSK7oLMobe"
   },
   "outputs": [],
   "source": [
    "# Set seed so we get the same random numbers\n",
    "np.random.seed(0)\n",
    "\n",
    "# Choose random values for the parameters\n",
    "omega_q = np.random.normal(size=(D,D))\n",
    "omega_k = np.random.normal(size=(D,D))\n",
    "omega_v = np.random.normal(size=(D,D))\n",
    "beta_q = np.random.normal(size=(D,1))\n",
    "beta_k = np.random.normal(size=(D,1))\n",
    "beta_v = np.random.normal(size=(D,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxaKQtP3Ng6R"
   },
   "source": [
    "Now let's compute the queries, keys, and values for each input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TwDK2tfdNmw9"
   },
   "outputs": [],
   "source": [
    "# Make three lists to store queries, keys, and values\n",
    "all_queries = []\n",
    "all_keys = []\n",
    "all_values = []\n",
    "# For every input\n",
    "for x in all_x:\n",
    "  # TODO -- compute the keys, queries and values.\n",
    "  # Replace these three lines\n",
    "  query = np.dot(omega_q, x)\n",
    "  key = np.dot(omega_k, x)\n",
    "  value = np.dot(omega_v, x)\n",
    "\n",
    "\n",
    "  all_queries.append(query)\n",
    "  all_keys.append(key)\n",
    "  all_values.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Se7DK6PGPSUk"
   },
   "source": [
    "We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u93LIcE5PoiM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(items_in):\n",
    "    # Exponentiate each item in the input list\n",
    "    exp_items = np.exp(items_in)\n",
    "    \n",
    "    # Divide each exponentiated item by the sum of all exponentiated items\n",
    "    items_out = exp_items / np.sum(exp_items)\n",
    "    \n",
    "    return items_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aJVhbKDW7lm"
   },
   "source": [
    "Now compute the self attention values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yimz-5nCW6vQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attentions for output 0\n",
      "[2.51131327e-10 9.99871742e-01 1.28257326e-04]\n",
      "Attentions for output 1\n",
      "[7.05249423e-08 7.32497391e-02 9.26750190e-01]\n",
      "Attentions for output 2\n",
      "[9.92197921e-01 6.37007031e-04 7.16507185e-03]\n",
      "x_prime_0_calculated: [[ 0.87981225 -0.54621062 -0.28669931 -0.08670637]]\n",
      "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
      "x_prime_1_calculated: [[ 1.52832666 -0.39795456  4.32644077  2.37090289]]\n",
      "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
      "x_prime_2_calculated: [[-2.74393541  3.22286389 -6.21268498 -2.63336572]]\n",
      "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume `all_queries`, `all_keys`, and `all_values` are defined, as well as `N` (number of queries) and `D` (dimension).\n",
    "# `all_keys` and `all_values` should be lists of numpy arrays where each array has shape (D, 1).\n",
    "# `N` represents the number of queries (outputs we want to generate).\n",
    "\n",
    "# Create an empty list for outputs\n",
    "all_x_prime = []\n",
    "\n",
    "# For each output (query)\n",
    "for n in range(N):\n",
    "    # Get the query vector for the nth output\n",
    "    query = all_queries[n]\n",
    "    \n",
    "    # Create list to store dot products of query `n` with all keys\n",
    "    all_km_qn = []\n",
    "    \n",
    "    # Compute the dot products of the query with each key\n",
    "    for key in all_keys:\n",
    "        # Compute dot product between the query and each key\n",
    "        dot_product = np.dot(query.T, key).item()  # .item() converts the result to a scalar\n",
    "        \n",
    "        # Store the computed dot product\n",
    "        all_km_qn.append(dot_product)\n",
    "\n",
    "    # Compute attention weights using softmax\n",
    "    attention = softmax(all_km_qn)\n",
    "    \n",
    "    # Print attention result (should be positive and sum to one)\n",
    "    print(\"Attentions for output\", n)\n",
    "    print(attention)\n",
    "\n",
    "    # Compute a weighted sum of all the values according to the attention weights\n",
    "    x_prime = np.zeros((D, 1))  # Initialize the weighted sum as a zero vector of shape (D, 1)\n",
    "    \n",
    "    # Sum the values weighted by attention\n",
    "    for i, value in enumerate(all_values):\n",
    "        x_prime += attention[i] * value  # Multiply each value by its attention weight and accumulate\n",
    "\n",
    "    # Store the computed x_prime\n",
    "    all_x_prime.append(x_prime)\n",
    "\n",
    "# Print calculated and true values to check correctness\n",
    "print(\"x_prime_0_calculated:\", all_x_prime[0].T)\n",
    "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
    "print(\"x_prime_1_calculated:\", all_x_prime[1].T)\n",
    "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
    "print(\"x_prime_2_calculated:\", all_x_prime[2].T)\n",
    "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ2vCQ_7C38K"
   },
   "source": [
    "Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n",
    "\n",
    "Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "obaQBdUAMXXv"
   },
   "outputs": [],
   "source": [
    "# Define softmax operation that works independently on each column\n",
    "def softmax_cols(data_in):\n",
    "  # Exponentiate all of the values\n",
    "  exp_values = np.exp(data_in) ;\n",
    "  # Sum over columns\n",
    "  denom = np.sum(exp_values, axis = 0);\n",
    "  # Replicate denominator to N rows\n",
    "  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n",
    "  # Compute softmax\n",
    "  softmax = exp_values / denom\n",
    "  # return the answer\n",
    "  return softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gb2WvQ3SiH8r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "    \"\"\"\n",
    "    Compute self-attention for a sequence in matrix form.\n",
    "\n",
    "    Parameters:\n",
    "    X       - Input sequence matrix of shape (N, D), where N is the number of inputs and D is the input dimension.\n",
    "    omega_v - Weight matrix for values, shape (D, D).\n",
    "    omega_q - Weight matrix for queries, shape (D, D).\n",
    "    omega_k - Weight matrix for keys, shape (D, D).\n",
    "    beta_v  - Bias vector for values, shape (D,).\n",
    "    beta_q  - Bias vector for queries, shape (D,).\n",
    "    beta_k  - Bias vector for keys, shape (D,).\n",
    "\n",
    "    Returns:\n",
    "    X_prime - Output sequence matrix of shape (N, D) after self-attention is applied.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute Queries, Keys, and Values\n",
    "    Q = X @ omega_q + beta_q.reshape(1, -1)  # Query matrix   \n",
    "    K = X @ omega_k + beta_k.reshape(1, -1)  # Key matrix\n",
    "    V = X @ omega_v + beta_v.reshape(1, -1)  # Value matrix\n",
    "\n",
    "\n",
    "    # Step 2: Compute Dot Products (Attention Scores)\n",
    "    # (N, D) x (D, N) -> (N, N)\n",
    "    attention_scores = Q @ K.T\n",
    "\n",
    "    # Step 3: Apply Softmax to Calculate Attention Weights\n",
    "    # Use softmax along each row to normalize the attention scores\n",
    "    attention_weights = softmax(attention_scores)\n",
    "\n",
    "    # Step 4: Weight Values by Attention Weights\n",
    "    # (N, N) x (N, D) -> (N, D)\n",
    "    X_prime = attention_weights @ V\n",
    "\n",
    "    return X_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MUOJbgJskUpl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04111728  0.119473    0.06520696 -0.19119696]\n",
      " [ 0.00421243 -0.55397478  0.34775034 -0.69706642]\n",
      " [-0.04731911 -0.7636419   0.2919839  -0.57878701]]\n"
     ]
    }
   ],
   "source": [
    "# Corrected dimensions for X\n",
    "X = np.zeros((N, D))\n",
    "\n",
    "# Assign values to each row of X\n",
    "X[0, :] = np.squeeze(all_x[0])\n",
    "X[1, :] = np.squeeze(all_x[1])\n",
    "X[2, :] = np.squeeze(all_x[2])\n",
    "\n",
    "# Run the self-attention mechanism\n",
    "X_prime = self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as_lRKQFpvz0"
   },
   "source": [
    "If you did this correctly, the values should be the same as above.\n",
    "\n",
    "TODO:  \n",
    "\n",
    "Print out the attention matrix\n",
    "You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kLU7PUnnqvIh"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_self_attention(X, omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product self-attention for a sequence in matrix form.\n",
    "\n",
    "    Parameters:\n",
    "    X       - Input sequence matrix of shape (N, D), where N is the number of inputs and D is the input dimension.\n",
    "    omega_v - Weight matrix for values, shape (D, D).\n",
    "    omega_q - Weight matrix for queries, shape (D, D).\n",
    "    omega_k - Weight matrix for keys, shape (D, D).\n",
    "    beta_v  - Bias vector for values, shape (D,).\n",
    "    beta_q  - Bias vector for queries, shape (D,).\n",
    "    beta_k  - Bias vector for keys, shape (D,).\n",
    "\n",
    "    Returns:\n",
    "    X_prime - Output sequence matrix of shape (N, D) after self-attention is applied.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute Queries, Keys, and Values\n",
    "    Q = X @ omega_q + beta_q.reshape(1, -1)  # Query matrix (N, D)\n",
    "    K = X @ omega_k + beta_k.reshape(1, -1)  # Key matrix (N, D)\n",
    "    V = X @ omega_v + beta_v.reshape(1, -1)  # Value matrix (N, D)\n",
    "\n",
    "    # Step 2: Compute Dot Products (Attention Scores)\n",
    "    attention_scores = Q @ K.T  # (N, N)\n",
    "\n",
    "    # Step 3: Scale the Dot Products\n",
    "    d_k = Q.shape[1]  # Dimensionality of the queries (or keys)\n",
    "    scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "\n",
    "    # Step 4: Apply Softmax to Calculate Attention Weights\n",
    "    attention_weights = softmax(scaled_scores)  # (N, N)\n",
    "\n",
    "    # Step 5: Compute Weighted Sum of Values\n",
    "    X_prime = attention_weights @ V  # (N, D)\n",
    "\n",
    "    return X_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "n18e3XNzmVgL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06844147  0.19846437  0.11080554 -0.30884349]\n",
      " [ 0.0437153  -0.29627934  0.32854197 -0.68906003]\n",
      " [-0.0146878  -0.48757334  0.24051127 -0.48450209]]\n"
     ]
    }
   ],
   "source": [
    "# Run the self attention mechanism\n",
    "X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
    "\n",
    "# Print out the results\n",
    "print(X_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDEkIrcgrql-"
   },
   "source": [
    "TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n",
    "If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOKrX9gmuhl9+KwscpZKr3u",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
