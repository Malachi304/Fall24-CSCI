{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzTUHNC0Oien"
      },
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nKf_jjk9OgN1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation, _= env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, trunicated ,info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation, _= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_q5-OvYOldL"
      },
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "cd7dd0a4-7b60-406d-a8c1-1f32b7f3088c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            " \n",
            " \n",
            " \n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, trunicated, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "U92a-f0HO1j7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Policy:\n",
            " [0 3 0 3 0 0 0 0 3 1 0 0 0 2 1 0]\n",
            "Optimal V: \n",
            "[0.0688909  0.06141457 0.07440976 0.05580732 0.09185454 0.\n",
            " 0.11220821 0.         0.14543635 0.24749695 0.29961759 0.\n",
            " 0.         0.3799359  0.63902015 0.        ]\n",
            "Average Reward: \n",
            " 0.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int) \n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # For each state in the environment\n",
        "        for state in range(env.observation_space.n):\n",
        "            # Initialize an array to store Q-values for all actions\n",
        "            Q_values = np.zeros(env.action_space.n)\n",
        "\n",
        "            # For each action in the current state, calculate the Q-value\n",
        "            for action in range(env.action_space.n):\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    # Implement the Bellman update to compute the Q-value for the current action\n",
        "                    Q_values[action] += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            # Update V[state] with the maximum Q-value (best action to take in this state)\n",
        "            V[state] = np.max(Q_values)\n",
        "\n",
        "            # Update the policy to take the action with the highest Q-value\n",
        "            policy[state] = np.argmax(Q_values)\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "\n",
        "# Apply Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "\n",
        "# Evaluate the optimal policy\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()  \n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _, _ = env.step(action)  \n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(f'Optimal Policy:\\n {optimal_policy}')\n",
        "print(f'Optimal V: \\n{optimal_V}')\n",
        "\n",
        "print(f'Average Reward: \\n {average_reward}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learned Policy: [0 3 0 0 0 0 2 0 3 1 0 0 0 2 1 0]\n",
            "Q Table:\n",
            " [[0.78235308 0.76323944 0.74521576 0.74639802]\n",
            " [0.56605467 0.31096697 0.43277967 0.70974904]\n",
            " [0.56767806 0.40492429 0.32293506 0.47018553]\n",
            " [0.39244805 0.06911799 0.02250614 0.07206191]\n",
            " [0.78731757 0.53722653 0.44319856 0.43830237]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.29681539 0.12781937 0.40549626 0.14848382]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.61442485 0.38835044 0.42355092 0.7981231 ]\n",
            " [0.43180419 0.78353947 0.42479198 0.65756888]\n",
            " [0.69833016 0.39997187 0.54790561 0.39005607]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.68530748 0.63629705 0.94108966 0.6004221 ]\n",
            " [0.88035729 1.00079521 0.98584869 0.93634979]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Average Reward:  0.76\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "\n",
        "def Q_learning(env, num_episodes=5000, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.9, min_epsilon=0.1):\n",
        "  \n",
        "    \"\"\"\n",
        "    Implements the Q-Learning algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        num_episodes: Number of iterations to run.\n",
        "        gamma: Discount factor.\n",
        "        Alpha: Learning rate\n",
        "        Epsilon: Exploration factor (1 = Agent's first step will always be in favor of exploration)\n",
        "        Epsilon_decay: Decay factor for epsilon (Decay factor closer to 1, means the agent will favor exploation early on, but explotation later down the line)\n",
        "        Min_epsilon: Even with decay, this ensures the agent will always explore sometimes (0.1 = 10% exploration after epsilon is decayed below 0.1)\n",
        "\n",
        "    Returns:\n",
        "        Q-Table, and the learned policy.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Initialize value function and policy\n",
        "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state,_ = env.reset()\n",
        "        done=False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.uniform(0,1) < epsilon:\n",
        "                # Exploration\n",
        "                action=env.action_space.sample()\n",
        "            else:\n",
        "                # Exploitation\n",
        "                action = np.argmax(Q[state, :])\n",
        "    \n",
        "            # Step\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            best_next_action = np.argmax(Q[next_state, :])\n",
        "\n",
        "            # Q-learning update rule\n",
        "            Q[state][action] = Q[state][action] + alpha*(reward + gamma*(Q[next_state][best_next_action] - Q[state][action]))\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "        # Update epsilon with decay\n",
        "        epsilon = max(min_epsilon, (epsilon*epsilon_decay))\n",
        "\n",
        "\n",
        "\n",
        "    # Policy takes action with highest Q-value\n",
        "    policy = np.argmax(Q, axis=1)\n",
        "\n",
        "    return Q, policy\n",
        "\n",
        "# Apply Value Iteration\n",
        "Q, learned_policy = Q_learning(env)\n",
        "\n",
        "# Evaluate the optimal policy\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state, _ = env.reset()  \n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _, _ = env.step(action)  \n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, learned_policy)\n",
        "print(f'learned Policy: {learned_policy}')\n",
        "print(f'Q Table:\\n {Q}')\n",
        "print(f'Average Reward:  {average_reward}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
